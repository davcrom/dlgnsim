\documentclass{article}
\usepackage[margin=2cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{enumitem}
\usepackage{comment}
\usepackage{proofread}
\usepackage[round]{natbib}
\bibliographystyle{plainnat}


\renewcommand{\familydefault}{\sfdefault}
\setlength{\parindent}{0pt}
\newcommand{\Lagr}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\vect}[1]{\textbf{\textrm{#1}}}
\newcommand{\pd}[2]{\frac{\partial {#1}}{\partial {#2}}}
\newcommand{\pdd}[2]{\frac{\partial^2 {#1}}{\partial {#2}}}
\renewcommand*{\arraystretch}{1.5}

\title{Working with the eDOG model: Stimulus optimization}
\author{Davide Crombie}

\begin{document}

\maketitle

\section{Introduction} \label{introduction}
The extended Difference of Gaussians (eDOG) is a mathematical model designed to capture the stimulus responses of the dorsal lateral geniculate nucleus of the thalamus (dLGN) via linear filtering operations that reflect the corticothalamic circuit structure \citep{einevoll2012, mobarhan2018}.
Under this model, the spatiotemporal response of a homogeneous population of neurons is given by the convolution of the input to the neural population (the stimulus, $S$) with the impulse-response function of the population ($G$).

\begin{equation} \label{impulseresp}
	R(\vect{r},t) = G(\vect{r},t)*S(\vect{r},t)
\end{equation}

By the Fourier-convolution theorem,

\begin{equation*}
	\hat{R} = \hat{G}\hat{S}
\end{equation*}

When the input is dependent on the response of a different, connected neural population, the input term is given by the convolution of the upstream population's response with a coupling kernel ($K$). 
The  model assumes that all such inputs sum linearly to produce a response.

\begin{equation*}
	\hat{R} = \sum_n{\hat{K}_n\hat{R}_n}
\end{equation*}

In the case of the dLGN relay neuron population, together with some simplifying assumptions that follow from experimental results, the response to a visual sitmulus is given by

\begin{align} \label{edog}
	\hat{R_r} & = \hat{K}_{rg}\hat{R}_g + \hat{K}_{rig}\hat{R}_g + 
				  \sum_n{\hat{K}_{rc_n}\hat{K}_{c_nr}\hat{R}_r} \nonumber \\
			  & = \frac{\hat{K}_{rg} + \hat{K}_{ri}}{1 - 
			  	  \sum_n{\hat{K}_{rc_n}\hat{K}_{c_nr}}} \hat{G}_g\hat{S} 
\end{align}

This model can be extended by making changes either to the parameters of the coupling kernels, impulse-response kernels, and/or the structure of the impulse-response term in (\ref{edog}). 
One might note, however, that the linear filter approach allows for mathematically defined stimuli as well as naturalistic stimuli to be used as input, and it is likely that part of this arbitrarily large stimulus space yeilds almost indistinguishible responses for two different model instances.

\begin{itemize}[label={}]
	\item \textbf{Goal:} find a stimulus on an arbitrary parameter space which maximally separates 	
		  the responses of two model configurations.
	\item \textbf{Approach:} iteratively optimize second-order approximations of the difference    
	      between the responses of two different model configurations by applying Newton's method 			  for root finding to the derivative of the difference function.
\end{itemize}


\section{Newton optimization} \label{newtonmethod}
In order to find the roots of a function $f(x)$, it is observed that, for $x$ sufficiently close, the root a line tangent to $f(x)$, $x^{(0)}$, is closer to the true root of $f$ than $x$ itself. 
This tangent line is defined as

\begin{equation} \label{tangent1d}
	g(x) = f'(x)(x^{(0)} - x) + f(x)
\end{equation}

Setting

\begin{equation*}
	g(x) = 0 \quad \longrightarrow \quad x^{(0)} = x - \frac{f(x)}{f'(x)}
\end{equation*}

This process can be performed iteratively

\begin{equation} \label{updaterule1d}
	x_{n+1} := x_n - \frac{f(x_n)}{f'(x_n)}
\end{equation}

If $f(x)$ is itself the gradient of another function $R$, the resultant $x^*$ will, if the algorithm has converged to a satifactory degree, represent a stationary point of $R$. 
In this manner, Newton's method for finding the roots of a function can be used in optimization problems. \par 
\hfill \newline
For a multi-dimensional vector-valued function $\vect{f}(\vect{x}): \R^n \rightarrow \R^n$, the tangent is defined by

\begin{align} \label{tangent}
	\vect{g}(\vect{x}) & = \vect{J}(\vect{x})(\vect{x}^{(0)} - \vect{x}) + \vect{f}(\vect{x})
\end{align}

Where $\vect{J}$ is the Jacobian of $\vect{f}$

\begin{equation*}
	\vect{J} = 
	\begin{pmatrix}
		\pd{\vect{f}(\vect{x})}{x_1} & \pd{\vect{f}(\vect{x})}{x_2} & \cdots 
		& \pd{\vect{f}(\vect{x})}{x_n}
	\end{pmatrix} = 
	\begin{pmatrix}
		\pd{f_1(\vect{x})}{x_1} & \pd{f_1(\vect{x})}{x_2} & \cdots & \pd{f_1(\vect{x})}{x_n} \\
		\pd{f_2(\vect{x})}{x_1} & \pd{f_2(\vect{x})}{x_2} & \cdots & \pd{f_2(\vect{x})}{x_n} \\
		\vdots & \vdots & \ddots & \vdots \\
		\pd{f_n(\vect{x})}{x_1} & \pd{f_n(\vect{x})}{x_2} & \cdots & \pd{f_n(\vect{x})}{x_n}
	\end{pmatrix}
\end{equation*}

For the roots given by $\vect{f} = \vect{0}$, the update rule becomes

\begin{equation} \label{updaterule}
	\vect{x}_{n+1} := \vect{x}_n - \vect{J}^{-1}(\vect{x}_n)\vect{f}(\vect{x}_n)	
\end{equation}

If the function $\vect{f}$ is itself the gradient of a function $R(\vect{x}): \R^n \rightarrow \R$, then the update rule becomes

\begin{equation} \label{updateruledog}
	\vect{x}_{n+1} := \vect{x}_n - \vect{H}_R^{-1}(\vect{x}_n)\nabla R(\vect{x}_n)
\end{equation}

where $\nabla R$ is the gradient of $R$ and $\vect{H}_R$ is the Hessian.
It is described in the appendix how this process is equivalent to iteratively optimizing a direct second order approximation of the response function $R$.


\section{Response function gradient} \label{responsegradient}
The function $R$ will be defined as

\begin{equation}
	R = (R_{r}^{\alpha} - R_{r}^{\beta})^2
\end{equation}

Where $R_{r}^{\alpha}$ is the Fourier-transformed spatio-temporal response of the relay cell population in an eDOG model instance $\alpha$. 
The full eDOG in (\ref{edog}) can be simplified back to the form of (\ref{impulseresp})

\begin{align}
	R_{r}^{\alpha}(\vect{r},t,\vect{x}) & = G_{r}^{\alpha}(\vect{r},t)*S(\vect{r},t,\vect{x}) 
	\nonumber \\[1em]
	\therefore \quad \hat{R}(\vect{k}, \omega, \vect{x}) & = 
	\left[ \left(\hat{G}^{\alpha}(\vect{k}, \omega) - \hat{G}^{\beta}(\vect{k}, \omega)\right)
	\hat{S}(\vect{k}, \omega, \vect{x}) \right]^2 \label{objfunc}
\end{align}

Thus, if the stimulus has a nice mathematical representation with a small number of parameters, $R$ and the gradient of $R$ w.r.t. $\vect{x}$ can be evaluated at a specific point $(\vect{r},t,\vect{x})$.
However, if $S$ is a "naturalistic stimulus", the entries in $\vect{x}$ refer to the intensity values of pixels in a static image or movie, finding an analytic solution to $R$, let alone computation of the gradient, is not possible.
Performing optimization directly on the complete Fourier-transformed response matrices, however, is more tractable, and produces expressions that can also be applied to mathematically-defined stimuli evaluated at a set of points in space and time.

\begin{align} \label{objmat}
	\hat{\vect{R}}(\vect{x}) & = \left( (\hat{\vect{G}}^\alpha - \hat{\vect{G}}^\beta) \circ 
						   		  \hat{\vect{S}}(\vect{x}) \right)^2 = 
						   		  \sum_{k}{\left( (\hat{g}_k^{\alpha} - 
						   		  \hat{g}_k^{\beta}) x_k \right)^2}
\end{align}

Assigning $g_k := \hat{g}_k^\alpha - \hat{g}_k^\beta$, the gradient and the Hessian are given by

\begin{align}
	\pd{\hat{\vect{R}}(\vect{x})}{\vect{x}} & = 
	\begin{pmatrix}
		2 g_1^2 x_1 & 2 g_2^2 x_2 & \cdots & 2 g_n^2 x_n
	\end{pmatrix}^{T} \label{objgradient} \\[1em]
	\pdd{\hat{\vect{R}}(\vect{x})}{\vect{x}^2} & =
	\begin{pmatrix}
		2 g_1^2 & 0 & \cdots & 0 \\
		0 & 2 g_2^2 & \cdots & 0 \\
		\vdots & \vdots\ & \ddots & \vdots \\
		0 & 0 & \cdots & 2g_n^2
	\end{pmatrix} \label{objhessian}
\end{align}

\section{Optimization constraints} \label{constraints}
The necessity for various constrains on the optimization problem arises from the above: it is clear from (\ref{objmat}) that the objective function can be arbitrarily large, depending on the amplitude of the stimulus. 
Thus the stimulus should be normalized for a meaningful result. \par
\hfill \newline
The Lagrangian method for constrained optimization seeks to find the maximum value of a function $f(\vect{x})$ constrained on the manifold $\lbrace \vect{x} \in \R^n \enspace | \enspace c(\vect{x}) = a \rbrace$ departing from the observation that at the maximum of $f(\vect{x})$ on $c(\vect{x}) = a$ the gradients of $f(\vect{x})$ and $c(\vect{x})$ are paralell \citep[ch.12]{nocedal2006}.

\begin{equation*}
	\nabla f(\vect{x}) = \lambda \nabla c(\vect{x}), \quad \text{where} \enspace c(\vect{x}) = a
\end{equation*}

The Lagrangian objective function is thus defined as

\begin{equation} \label{lagrangian}
	\Lagr(\vect{x},\lambda) = f(\vect{x}) - \lambda(c(\vect{x}) - a)
\end{equation}

\begin{comment}
Subject to multiple constraints

\begin{equation} \label{lagrangiannd}
	\Lagr(\vect{x}, \bm{\lambda}) = f(\vect{x}) - \sum_k{\lambda_k(c_k(\vect{x}) - a_k)}
\end{equation}
\end{comment}

Formally, the stimulus amplitude constraint set is given by the unit $n$-sphere ($L_2$ norm)

\begin{align}
	\lbrace \vect{x} \in \R^n \enspace | \enspace c(\vect{x}) = 0 \rbrace, \quad
	\text{where} \enspace c(\vect{x}) = ||\vect{x}||_2 - 1 = \sum_n{x_n^2} - 1
\end{align}

The gradient is given by

\begin{align}
	\nabla \Lagr(\vect{x},\bm{\lambda}) & = 
	\begin{pmatrix}
		\pd{\Lagr}{\lambda} & \pd{\Lagr}{\vect{x}_1} & \pd{\Lagr}{\vect{x}_2} 
		& \cdots & \pd{\Lagr}{\vect{x}_n}
	\end{pmatrix}^{T} \\
	& = 
	\begin{pmatrix}
		c(\vect{x}) \\
		\pd{f(\vect{x})}{x_1} + \lambda \pd{c(\vect{x})}{x_1} \\
		\pd{f(\vect{x})}{x_2} + \lambda \pd{c(\vect{x})}{x_2}	\\	
		\vdots \\
		\pd{f(\vect{x})}{x_n} + \lambda \pd{c(\vect{x})}{x_n}
	\end{pmatrix} = 
	\begin{pmatrix}
		\sum_n{x_n^2} - 1 \\
		2 g_1^2 x_1 + 2 \lambda x_1) \\
		2 g_1^2 x_2 + 2 \lambda x_2) \\
		\vdots \\
		2 g_1^2 x_n + 2 \lambda x_n)
	\end{pmatrix} \nonumber
\end{align}

And the bordered Hessian is given by

\begin{align}
	\widetilde{\vect{H}}_\Lagr & = 
	\begin{pmatrix}
		\pdd{\Lagr}{\lambda^2} & \pdd{\Lagr}{\lambda\vect{x}} \\
		\pdd{\Lagr}{\vect{x}\lambda} & \pdd{\Lagr}{\vect{x}^2}
	\end{pmatrix} = 
	\begin{pmatrix}
		0 & \pdd{\Lagr}{\lambda\vect{x}} \\
		\pdd{\Lagr}{\vect{x}\lambda} & \pdd{\Lagr}{\vect{x}^2}
	\end{pmatrix} \\
	& = 
	\begin{pmatrix}
		0 & 2x_1 & 2x_2 & \cdots & 2x_n \\
		2x_1 & 2 (g_1^2 + \lambda) & 0 & \cdots & 0 \\
		2x_2 & 0 & 2 (g_2^2 + \lambda) & \cdots & 0 \\
		\vdots & \vdots & \vdots & \ddots & \vdots \\
		2x_n & 0 & 0 & \cdots & 2 (g_n^2 + \lambda)
	\end{pmatrix} \nonumber
\end{align}

However, it is noted that the system of equations given by $\nabla \Lagr(\vect{x},\bm{\lambda}) = \vect{0}$ includes non-linear terms, and thus $(\lambda, x_1, x_2, ... , x_n)$ cannot be solved for.
If $c(\vect{x})$ is re-defined as the $L_1$ norm of the stimulus vector (i.e. the plane $c(\vect{x}) = ||\vect{x}||_1 = \sum_n{|x_n|} - 1$), then a linear system is obtained which can be solved algorithmically (e.g. Gauss-Jordan elimination) without losing the main purpose of the constraint.
However, the plane defined by this constraint is not smooth, and therefore the equality constriant $c(\vect{x}) = \sum_n{|x_n|} - 1$ has to be split up into a set of inequality constraints \citep[ch.12, p. 306]{nocedal2006}. \yel{[This needs to be worked out further.]}


\section{A note on implementation}
Rather than being incorporated into mathematical formalism, the constraint on stimulus amplitude can also be implemented by re-normalizing the stimulus after every iteration of the optimizer. Geometrically, this has the effect of moving the new point $\vect{x}_{n+1}$ onto the sphere $\sum_n{x_n^2} = 1$ without changing the direction of the vector. \par
\hfill \newline
Additional, more general, constraints on the Newton optimization method regard properties of the Hessian matrix.
From (\ref{updateruledog}) it can be seen that the response function Hessian must be invertible, that is

\begin{equation}
	det(H_R) \neq 0
\end{equation}

Further, if a maximum is required, the Hessian must be negative definite (all eigenvalues are $< 0$).
The first condition implies that the algorithm will fail if $\vect{x}$ falls on an inflection point, and the second condition is necessary for the search direction to point towards a maximum. 
Both of these conditions can be dealt with by re-intializing the optimization from a new starting point when one of the conditions is violated.
This method should be implemented even when the algorithm successfully terminates, as the search method does not guarantee that a global maximum will be found. 

\section{A worked example}
\cite{mobarhan2018} have shown that an eDOG network configuration where inhibitory cortical feedback is delayed compared to excitatory feedback shifts the temporal filtering properties of dLGN relay cell responses to drifting grating stimuli from low-pass (when feedback weights are 0) to band-pass (for increasing weights). Presently, data is being collected for dLGN relay cell responses to a "full-field chirp" stimulus with via optogenetic suppression of cortical feedback. Previous results \citep{mobarhan2018} suggest that the response matrix to a chirp stimulus would contain higher power in at some peak temporal frequency when cortical feedback is present. It can be seen from (\ref{objfunc}) that the response to a certain angular frequency $\omega$ present in the stimulus is proportional to the impulse-response function $\hat{G}$ evaluated at $\omega$. Let $\hat{G}_r^\alpha$ represent the response of a model instance with feedback present, and $\hat{G}_r^\beta$ represent the response of a model instance without feedback.

\begin{align*}
	\hat{G}_r^\alpha & = \frac{\hat{K}_{rg} + \hat{K}_{rig}}
					   {1 - \hat{K}_{rcr}^{ex} - \hat{K}_{rcr}^{in}} \hat{G}_g \\
	\hat{G}_r^\beta & = \left( \hat{K}_{rg} + \hat{K}_{rig} \right) \hat{G}_g \\
\end{align*}

The objective function is given by

\begin{equation}
	\hat{G}(\omega) = \hat{G}_r^\alpha - \hat{G}_r^\beta = 
	\left( \frac{1}{1 - \hat{K}_{rcr}^{ex} - \hat{K}_{rcr}^{in}} - 1 \right)
	\left( \hat{K}_{rg} + \hat{K}_{rig} \right) \hat{G}_g
\end{equation}

\yel{[...]}

\section{Appendix} \label{appendix}

\subsection{A note on the partial derivatives of scalar-valued functions}
The following convention for the gradient ($\nabla$), Jacobian ($\vect{J}$), and Hessian($\vect{H}$) of a function $f: \R^n \rightarrow \R$ is adopted

\begin{align}
	\pd{f(\vect{x})}{\vect{x}} := \quad \nabla f & = 
	\begin{pmatrix}
		\pd{f(\vect{x})}{x_1} & \pd{f(\vect{x})}{x_2} & \cdots & \pd{f(\vect{x})}{x_n}
	\end{pmatrix}^{T} \label{gradient} \\[1em]
	\vect{J}_f & = 
	\begin{pmatrix}
		\pd{f(\vect{x})}{x_1} & \pd{f(\vect{x})}{x_2} & \cdots & \pd{f(\vect{x})}{x_n}
	\end{pmatrix} \label{jacobian} \\[1em]
	\vect{H}_f & = \pd{}{\vect{x}} \left( \pd{f(\vect{x})}{\vect{x}} \right) \nonumber \\
	& = \vect{J}_f(\nabla f)^{T} \label{hessian}
\end{align}	

\subsection{Identities for gradients of vectorized scalar forms}
Given the scalar $f(\vect{x}) = \vect{a}^{T}\vect{x}$, where

\begin{equation*}
	\vect{x}= 
	\begin{pmatrix}
		x_1 & x_2 & \cdots & x_n
	\end{pmatrix}^{T}
	\qquad \vect{a} = 
	\begin{pmatrix}
		a_1 & a_2 & \cdots & a_n
	\end{pmatrix}^{T}
\end{equation*}

$f(\vect{x})$ expands to

\begin{equation*}
	f(\vect{x}) = a_1x_1 + a_2x_2 + ... + a_nx_n
\end{equation*}

And by (\ref{gradient})

\begin{equation*}
	\pd{f(\vect{x})}{\vect{x}} = 
	\begin{pmatrix}
		a_1 & a_2 & \cdots & a_n
	\end{pmatrix}^{T} 
	= \vect{a}
\end{equation*}

Because $\vect{a}^{T}\vect{x} = \vect{x}^{T}\vect{a}$,

\begin{equation} \label{scalargradvv}
	\pd{\vect{a}^{T}\vect{x}}{\vect{x}} = \pd{\vect{x}^{T}\vect{a}}{\vect{x}} = \vect{a} \\[2em]
\end{equation} 

Given the scalar $f(\vect{x}) = \vect{x}^{T}\vect{A}\vect{x}$, where

\begin{equation*}
	\vect{x}= 
	\begin{pmatrix}
		x_1 \\ 
		x_2 \\ 
		\vdots \\ 
		x_n
	\end{pmatrix}
	\qquad \vect{A} = 
	\begin{pmatrix}
		a_{11} & a_{12} & \cdots & a_{1n} \\
		a_{21} & a_{22} & \cdots & a_{2n} \\
		\vdots & \vdots & \ddots & \vdots \\
		a_{n1} & a_{n2} & \cdots & a_{nn}
	\end{pmatrix}
\end{equation*}

$f(\vect{x})$ expands to

\begin{align*}
	f(\vect{x}) = \, & a_{11}x_1^2 + a_{12}x_2x_1 + ... + a_{1n}x_nx_1 + \\
				  & a_{21}x_1x_2 + a_{22}x_2^2 + ... + a_{2n}x_nx_2 + ... \, + \\
				  & a_{11}x_1x_n + a_{12}x_2x_n + ... + a_{nn}x_n^2
\end{align*}

The partial derivative with respect to $\vect{x}$ is given by (\ref{gradient}), where

\begin{equation*}
	\begin{matrix}
		\pd{f(\vect{x})}{x_1} = (a_{11} + a_{11})x_1 + (a_{12} + a_{21})x_2 + ... + 
							   	(a_{1n} + a_{n1})x_n \\
		\pd{f(\vect{x})}{x_2} = (a_{21} + a_{12})x_1 + (a_{22} + a_{22})x_2 + ... + 
							   	(a_{2n} + a_{n2})x_n \\
		\vdots \\
		\pd{f(\vect{x})}{x_n} = (a_{n1} + a_{1n})x_1 + (a_{n2} + a_{2n})x_2 + ... + 
							   	(a_{nn} + a_{nn})x_n 
	\end{matrix}
\end{equation*}

In this form, it is clear that

\begin{align*}
	\pd{f(\vect{x})}{\vect{x}} & = 
	\begin{pmatrix}
		(a_{11} + a_{11}) & (a_{12} + a_{21}) & \cdots & (a_{1n} + a_{n1}) \\
		(a_{21} + a_{12}) & (a_{22} + a_{22}) & \cdots & (a_{2n} + a_{n2}) \\
		\vdots & \vdots & \ddots & \vdots \\
		(a_{n1} + a_{1n}) & (a_{n2} + a_{2n}) & \cdots & (a_{nn} + a_{nn}) 
	\end{pmatrix} 
	\begin{pmatrix}
		x_1 \\ 
		x_2 \\ 
		\vdots \\ 
		x_n
	\end{pmatrix} \\
	& = (\vect{A} + \vect{A}^{T})\vect{x}
\end{align*}

Thus

\begin{equation} \label{scalargradqf}
	\pd{\vect{x}^{T}\vect{Ax}}{\vect{x}} = (\vect{A} + \vect{A}^{T})\vect{x}
\end{equation}

\subsection{Equivalence of second-order approximation and Newton optimization methods}
The second-order Taylor expansion of a twice-differentiable function $f$ at a point $x \rightarrow a$ is given by

\begin{equation} \label{taylor1d}
	f(x) \approx f(a) + f'(a)(x - a) + \frac{1}{2}f''(a)(x - a)^2
\end{equation}

For $f: \R^2 \rightarrow \R$, this becomes

\begin{align} \label{taylor2d}
	f(x,y) \approx \, & f(a,b) + f_x(a,b)(x - a) + f_y(a,b)(y - b) \, + \nonumber \\
					   & \frac{1}{2}f_{xx}(a,b)(x - a)^2 + \frac{1}{2}f_{xy}(a,b)(x - a)(y - b) + 
						 \frac{1}{2}f_{yy}(a,b)(y - a)^2 
\end{align}
 
Vectorizing (\ref{taylor2d}) and generalizing to $n$ dimensions, $f: \R^n \rightarrow \R$ is approximated by

\begin{equation} \label{taylornd}
	f(\vect{x}) \approx f(\vect{a}) + \vect{J}(\vect{a})(\vect{x} - \vect{a}) + 
						 \frac{1}{2}(\vect{x} - \vect{a})^{T}\vect{H}(\vect{a})(\vect{x} - \vect{a})
\end{equation}

Where $\vect{J}$ is the Jacobian of $f$, and $\vect{H}$ is the Hessian (which is assumed to be symmetrical by the Schwarz-Clairaut theorem). 
Expanding, and collecting all terms dependent on $\vect{x}$

\begin{equation*}
	f(\vect{x}) \approx \frac{1}{2}\vect{x}^{T}\vect{Hx} - \frac{1}{2}\vect{x}^{T}\vect{Ha} -
						 \frac{1}{2}\vect{a}^{T}\vect{Hx} + \vect{Jx} + ...
\end{equation*}

In order to find stationary points, the derivative is taken with respect to $\vect{x}$. 
By (\ref{scalargradqf}) and (\ref{scalargradvv}) respectively, the partial derivatives of the first and fourth terms with respect to $\vect{x}$ become

\begin{align*}
	\pd{\vect{x}^{T}\vect{Hx}}{\vect{x}} & = (\vect{H} + \vect{H}^{T})\vect{x} \\
	\pd{\vect{Jx}}{\vect{x}} = \pd{(\vect{J}^{T})^{T}\vect{x}}{\vect{x})} & = \vect{J}^{T}
\end{align*}
Next, it is noted that

\begin{align*}
	\vect{A}^{T}\vect{B}^{T} & = (\vect{BA})^{T} \\
	\vect{H} & = \vect{H}^{T}
\end{align*}
	
Thus, if $\vect{z} = \vect{Ha} \enspace \rightarrow \enspace \vect{a}^{T}\vect{H} = \vect{z}^{T}$, and

\begin{equation*}
	\pd{\vect{z}^{T}\vect{x}}{\vect{x}} = \pd{\vect{x}^{T}\vect{z}}{\vect{x}} = 
	\vect{z} = \vect{Ha}
\end{equation*}

Finally, assembling the partial derivatives of all terms

\begin{equation}
	\pd{f(\vect{x})}{\vect{x}} = \vect{Hx} - \vect{Ha} + \vect{J}^{T}
\end{equation}

Setting

\begin{equation*}
	\pd{f(\vect{x})}{\vect{x}} = 0 \quad \longrightarrow \quad 
	\vect{x} = \vect{a} - \vect{H}^{-1}\vect{J}^{T}
\end{equation*}

Algorithmically,

\begin{equation}
	\vect{x}_{n+1} := \vect{x}_n - \vect{H}^{-1}(\vect{x}_n)\vect{J}^{T}(\vect{x}_n) \label{updaterule2}
\end{equation}

\subsection{Expansion of eDOG model terms}
Kernels are assumed to have separable space and time components and a weight \citep{einevoll2012, mobarhan2018} 

\begin{align*}
	G(r,t) & = w f(r) h(t)
\end{align*}

Impulse-response functions of retinal gangion cells ($G_g$)

\begin{align*}
	f_g(r) & = \frac{A}{\pi a^2} e^{\left( \frac{-r}{a} \right)^2} - 
			 \frac{B}{\pi b^2} e^{\left( \frac{-r}{b} \right)^2} \\
	h_g & = \left \lbrace
	\begin{array}{r@{}l}
		\sin(\frac{\pi t}{\tau}), & \quad 0 \leq t \leq \tau \\
		B \sin(\frac{\pi t}{\tau}), & \quad \tau < t \leq 2 \tau \\
		0, & \quad \text{else}
	\end{array} \right.
\end{align*}

Coupling kernel $K$ of input population $n$

\begin{align*}
	f_n(r) & = \frac{A_n}{\pi a_n^2} e^{\left( \frac{-r}{a_n} \right)^2} \\
	h_n & = \frac{1}{\tau_n} e^{-\frac{t - \Delta_n}{\tau_n}} \theta(t - \Delta_n)
\end{align*}


\bibliography{references.bib}

\end{document}