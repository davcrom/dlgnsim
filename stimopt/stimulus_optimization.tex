\documentclass{article}
\usepackage[margin=2cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{comment}
\usepackage{proofread}
\usepackage[round]{natbib}
\bibliographystyle{plainnat}


\renewcommand{\familydefault}{\sfdefault}
\setlength{\parindent}{0pt}
\newcommand{\Lagr}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\vect}[1]{\textbf{\textrm{#1}}}
\newcommand{\pd}[2]{\frac{\partial {#1}}{\partial {#2}}}
\newcommand{\pdd}[2]{\frac{\partial^2 {#1}}{\partial {#2}}}
\renewcommand*{\arraystretch}{1.5}

\title{Working with the eDOG model: Stimulus optimization}
\author{Davide Crombie}

\begin{document}

\maketitle

\section{Introduction} \label{introduction}
The extended Difference of Gaussians (eDOG) is a mathematical model designed to capture the stimulus responses of the dorsal lateral geniculate nucleus of the thalamus (dLGN) via linear filtering operations that reflect the corticothalamic circuit structure \citep{einevoll2012, mobarhan2018}.
Under this model, the spatiotemporal response of a homogeneous population of neurons is given by the convolution of the input to the neural population (the stimulus, $S$) with the impulse-response function of the population ($G$).

\begin{equation} \label{impulseresp}
	R(\vect{r},t) = G(\vect{r},t)*S(\vect{r},t)
\end{equation}

By the Fourier-convolution theorem,

\begin{equation*}
	\hat{R} = \hat{G}\hat{S}
\end{equation*}

When the input is dependent on the response of a different, connected neural population, the input term is given by the convolution of the upstream population's response with a coupling kernel ($K$). 
The  model assumes that all such inputs sum linearly to produce a response.

\begin{equation*}
	\hat{R} = \sum_n{\hat{K}_n\hat{R}_n}
\end{equation*}

In the case of the dLGN relay neuron population, together with some simplifying assumptions that follow from experimental results, the response to a visual sitmulus is given by

\begin{align} \label{edog}
	\hat{R_r} & = \hat{K}_{rg}\hat{R}_g + \hat{K}_{rig}\hat{R}_g + 
				  \sum_n{\hat{K}_{rc_n}\hat{K}_{c_nr}\hat{R}_r} \nonumber \\
			  & = \frac{\hat{K}_{rg} + \hat{K}_{ri}}{1 - 
			  	  \sum_n{\hat{K}_{rc_n}\hat{K}_{c_nr}}} \hat{G}_g\hat{S} 
\end{align}

This model can be extended by making changes either to the parameters of the coupling kernels, impulse-response kernels, and/or the structure of the impulse-response term in \eqref{edog}. 
One might note, however, that the linear filter approach allows for mathematically defined stimuli as well as naturalistic stimuli to be used as input, and it is likely that part of this arbitrarily large stimulus space yeilds almost indistinguishible responses for two different model instances.

\begin{center}
\begin{minipage}{0.8\textwidth}
	\textbf{Goal:} find a stimulus on an arbitrary parameter space which maximally separates 	
	the responses of two model configurations.
\end{minipage}
\end{center}


\section{Response function gradient} \label{responsegradient}
Let the objective function $R$ be defined as

\begin{equation}
	R = (R_{r}^{\alpha} - R_{r}^{\beta})^2
\end{equation}

Where $R_{r}^{\alpha}$ is the spatio-temporal response of the relay cell population in an eDOG model instance $\alpha$. 
The full eDOG in \eqref{edog} can be simplified back to the form of \eqref{impulseresp}

\begin{equation*}
	R_{r}^{\alpha}(\vect{r},t,\vect{x}) = G_{r}^{\alpha}(\vect{r},t)*S(\vect{r},t,\vect{x}) 
\end{equation*}

\begin{equation} \label{objfunc}
	\therefore \quad R(\vect{r}, t, \vect{x}) = \mathcal{F}^{-1} \left[ 
	\left(\hat{G}^{\alpha}(\vect{k}, \omega) - \hat{G}^{\beta}(\vect{k}, \omega)\right)
	\hat{S}(\vect{k}, \omega, \vect{x}) \right]^2 
\end{equation}

Evaluating the gradient of this function with respect to the stimulus parameters $\vect{x}$ - as is necessary for optimization - is not possible.
Furthermore, this formulation of response functions precludes the use of a "naturalistic stimulus", where the entries in $\vect{x}$ would refer to refer to the intensity values of pixels in an image.
As in a digital image, impulse-response functions and mathematically-defined stimuli can be evaluated at a discrete set of points in space and time.
When all terms are treated as vectors with independent elements:

\begin{align} \label{objmat}
	\hat{\vect{R}}(\vect{x}) & = \left( (\hat{\vect{G}}^\alpha - \hat{\vect{G}}^\beta) \circ 
						   		  \vect{x} \right)^2 = 
						   		  \left( \hat{\vect{G}} \circ  \vect{x} \right)^2 = 
						   		  \sum_n{\left( \hat{g}_n x_n \right)^2}
\end{align}

Where $\hat{\vect{R}}(\vect{x}) = \vect{R}(\vect{x})$ by Perseval's theorem.
The gradient and the Hessian are then given by

\begin{align}
	\pd{\hat{\vect{R}}(\vect{x})}{\vect{x}} & = 
	\begin{pmatrix}
		2 g_1^2 x_1 & 2 g_2^2 x_2 & \cdots & 2 g_n^2 x_n
	\end{pmatrix}^{T} \label{objgradient} \\[1em]
	\pdd{\hat{\vect{R}}(\vect{x})}{\vect{x}^2} & =
	\begin{pmatrix}
		2 g_1^2 & 0 & \cdots & 0 \\
		0 & 2 g_2^2 & \cdots & 0 \\
		\vdots & \vdots\ & \ddots & \vdots \\
		0 & 0 & \cdots & 2g_n^2
	\end{pmatrix} \label{objhessian}
\end{align}

\section{First-order constraints} \label{foc}
The necessity for various constrains on the optimization problem arises from the above: it is clear from \eqref{objmat} that the objective function can be arbitrarily large, depending on the amplitude of the stimulus. 
Thus the stimulus should be normalized for a meaningful result. \par
\hfill \newline
The Lagrangian method for constrained optimization seeks to find the maximum value of a function $f(\vect{x})$ constrained on the manifold $\lbrace \vect{x} \in \R^n \enspace | \enspace c(\vect{x}) = a \rbrace$ departing from the observation that at the maximum of $f(\vect{x})$ on $c(\vect{x}) = a$ the gradients of $f(\vect{x})$ and $c(\vect{x})$ are paralell \citep[ch.12]{nocedal2006}.

\begin{equation*}
	\nabla f(\vect{x}) = \lambda \nabla c(\vect{x}), \quad \text{where} \enspace c(\vect{x}) = a
\end{equation*}

The Lagrangian function is thus constructed as

\begin{equation} \label{lagrangian}
	\Lagr(\vect{x},\lambda) = f(\vect{x}) - \lambda(c(\vect{x}) - a)
\end{equation}

Formally, the stimulus amplitude constraint set is given by the unit $n$-sphere (sometimes referred to as the $L_2$ norm of $\vect{x}$)

\begin{equation} \label{constraint}
	c(\vect{x}) = ||\vect{x}||_2 - 1 = \sum_n{x_n^2} - 1 = 0
\end{equation}

So the Lagrangian objective function given \eqref{objmat} and \eqref{constraint} is

\begin{equation}
	\Lagr_R(\vect{x},\lambda) = \hat{\vect{R}}(\vect{x}) - \lambda c(\vect{x})
\end{equation}

The gradient is given by

\begin{align} \label{lagrgrad}
	\widetilde{\nabla} \Lagr(\vect{x},\bm{\lambda}) & = 
	\begin{pmatrix}
		\pd{\Lagr}{\lambda} & \pd{\Lagr}{\vect{x}_1} & \pd{\Lagr}{\vect{x}_2} 
		& \cdots & \pd{\Lagr}{\vect{x}_n}
	\end{pmatrix}^{T} \\
	\widetilde{\nabla} \Lagr_R(\vect{x},\bm{\lambda})& = 
	\begin{pmatrix}
		c(\vect{x}) \\
		\pd{f(\vect{x})}{x_1} - \lambda \pd{c(\vect{x})}{x_1} \\
		\pd{f(\vect{x})}{x_2} - \lambda \pd{c(\vect{x})}{x_2}	\\	
		\vdots \\
		\pd{f(\vect{x})}{x_n} - \lambda \pd{c(\vect{x})}{x_n}
	\end{pmatrix} = 
	\begin{pmatrix}
		\sum_n{x_n^2} - 1 \\
		2 g_1^2 x_1 - 2 \lambda x_1 \\
		2 g_2^2 x_2 - 2 \lambda x_2 \\
		\vdots \\
		2 g_n^2 x_n - 2 \lambda x_n
	\end{pmatrix} \nonumber
\end{align}

The Lagrangian objective has stationary points at $\nabla \Lagr(\vect{x},\bm{\lambda}) = \vect{0}$.
The bottom $n$ rows of this system of equations form the eigenvalue problem

\begin{equation}
	\vect{I} \hat{\vect{G}} \vect{x} = \lambda \vect{x}
\end{equation}

The trivial solution to this eigenvalue problem, $\vect{x} = \vect{0}$ does not satisfy $c(\vect{x}) = 0$, and hence is not a solution to the full $\nabla \Lagr(\vect{x},\bm{\lambda}) = \vect{0}$ system.
The non-trival solutions, given by the elements of $\hat{\vect{G}}$ and the corresponding standard basis vectors of $\R^n$ (e.g. $\mu = g_1$ and $\vect{x} = (1, 0, ..., 0))$, are valid as they do satisfy the constraint. 
Thus the objective function $\hat{\vect{R}}$ has stationary points at values of $\vect{x}$ corresponding to pure spatiotemporal sine waves.
It can be seen from \eqref{objmat} that the maximum of all the stationary points occurs at $\text{argmax}(\hat{\vect{G}})$.
The curvature at this stationary point, however, remains to be determined.


\section{Second-order constraints} \label{soc}
If the goal is to maximize the objective function, it must be ensured that the curvature of the function is negative at the valid stationary points. While the full bordered Hessian is given by

\begin{equation} \label{lagrhessian}
	\widetilde{\vect{H}}_\Lagr = 
	\begin{pmatrix}
		\pdd{\Lagr}{\lambda^2} & \pdd{\Lagr}{\lambda\vect{x}} \\
		\pdd{\Lagr}{\vect{x}\lambda} & \pdd{\Lagr}{\vect{x}^2}
	\end{pmatrix} = 
	\begin{pmatrix}
		0 & \pdd{\Lagr}{\lambda\vect{x}} \\
		\pdd{\Lagr}{\vect{x}\lambda} & \pdd{\Lagr}{\vect{x}^2}
	\end{pmatrix}
\end{equation}

The relevant curvature is given more simply by

\begin{equation}
	\vect{H}_{\Lagr_R} =
	\begin{pmatrix}
		2g_1^2 - 2\lambda & 0 & \cdots & 0 \\
		0 & 2g_2^2 - 2\lambda & \cdots & 0 \\
		\vdots & \vdots & \ddots & \vdots \\
		0 & 0 & \cdots & 2g_n^2 - 2\lambda
	\end{pmatrix} \nonumber
\end{equation}

The curvature of the constrained objective can be characterized by the eigenvalues of this Hessian, which are given by the elements of the diagonal.
Replacing $\lambda$ with $g_*^2$ (where $g_* = \text{max}(\hat{\vect{G}})$), a set of eigenvlaues is obtained that are negative for all values corresponding to $g_i \leq g_*$ and zero for values corresponding to $g_i = g*$.
Therefore $\Lagr_R$ is maximized with respect to all stimulus dimensions except for $\vect{x}_*$, where there is no curvature. \par
\hfill \newline
It can be concluded that the stimulus which maximizes the difference between any two eDOG model instances is the spatiotemporal sine wave at which the difference in the value of the two impulse-response function, evaluated at that spatiotemporal frequency, is greatest.

\begin{comment}
If $c(\vect{x})$ is re-defined as the $L_1$ norm of the stimulus vector (i.e. the plane $c(\vect{x}) = ||\vect{x}||_1 = \sum_n{|x_n|} - 1$), then a linear system is obtained which can be solved algorithmically (e.g. Gauss-Jordan elimination) without losing the main purpose of the constraint.
However, the plane defined by this constraint is not smooth, and therefore the equality constriant $c(\vect{x}) = \sum_n{|x_n|} - 1$ has to be split up into a set of inequality constraints \citep[ch.12, p. 306]{nocedal2006}. \yel{[This needs to be worked out further.]}
\end{comment}


\section{Temporal frequency preferences}
\cite{mobarhan2018} have shown that an eDOG network configuration where inhibitory cortical feedback is delayed compared to excitatory feedback shifts the temporal filtering properties of dLGN relay cell responses to drifting grating stimuli from low-pass (when feedback weights are 0) to band-pass (for increasing feedback weights). 
Presently, data is being collected for dLGN relay cell responses to a "full-field chirp" stimulus with optogenetic suppression of cortical feedback. 
The results from \cite{mobarhan2018} suggest that the response matrix to a chirp stimulus would contain higher power at some peak temporal frequency when cortical feedback is present. 
Let $\hat{R}_r^\alpha$ represent the response of a model instance with feedback present, and $\hat{R}_r^\beta$ represent the response of a model instance without feedback.
The corresponding impulse-response functions are given by

\begin{align*}
	\hat{G}_r^\alpha & = \frac{\hat{K}_{rg} + \hat{K}_{rig}}
					   {1 - \hat{K}_{rcr}^{ex} - \hat{K}_{rcr}^{in}} \hat{G}_g \\
	\hat{G}_r^\beta & = \left( \hat{K}_{rg} + \hat{K}_{rig} \right) \hat{G}_g \\
\end{align*}

And the stimulus which maximally separates the two responses is the sine wave pointing in the direction of

\begin{equation*}
	\text{argmax}\left( \hat{\vect{G}}_r^\alpha - \hat{\vect{G}}_r^\beta \right)
\end{equation*}

\yel{[depictions of the impulse-response matrices, their difference, and the optimal stimulus will go here]}


\section{Response nonlinearities}
\yel{[...]}

In cases where the system of equations given by $\nabla \Lagr(\vect{x},\bm{\lambda}) = \vect{0}$ cannot be solved analytically, a numerical approach must be adopted for optimization.

\section{Newton optimization} \label{newtonmethod}
In order to find the roots of a function $f(x)$, it is observed that, for $x$ sufficiently close, the root a line tangent to $f(x)$, $x^{(0)}$, is closer to the true root of $f$ than $x$ itself. 
This tangent line is defined as

\begin{equation} \label{tangent1d}
	g(x) = f'(x)(x^{(0)} - x) + f(x)
\end{equation}

Setting

\begin{equation*}
	g(x) = 0 \quad \longrightarrow \quad x^{(0)} = x - \frac{f(x)}{f'(x)}
\end{equation*}

This process can be performed iteratively

\begin{equation} \label{updaterule1d}
	x_{n+1} := x_n - \frac{f(x_n)}{f'(x_n)}
\end{equation}

If $f(x)$ is itself the gradient of another function $R$, the resultant $x^*$ will, if the algorithm has converged to a satifactory degree, represent a stationary point of $R$. 
In this manner, Newton's method for finding the roots of a function can be used in optimization problems. \par 
\hfill \newline
For a multi-dimensional vector-valued function $\vect{f}(\vect{x}): \R^n \rightarrow \R^n$, the tangent is defined by

\begin{align} \label{tangent}
	\vect{g}(\vect{x}) & = \vect{J}(\vect{x})(\vect{x}^{(0)} - \vect{x}) + \vect{f}(\vect{x})
\end{align}

Where $\vect{J}$ is the Jacobian of $\vect{f}$

\begin{equation*}
	\vect{J} = 
	\begin{pmatrix}
		\pd{\vect{f}(\vect{x})}{x_1} & \pd{\vect{f}(\vect{x})}{x_2} & \cdots 
		& \pd{\vect{f}(\vect{x})}{x_n}
	\end{pmatrix} = 
	\begin{pmatrix}
		\pd{f_1(\vect{x})}{x_1} & \pd{f_1(\vect{x})}{x_2} & \cdots & \pd{f_1(\vect{x})}{x_n} \\
		\pd{f_2(\vect{x})}{x_1} & \pd{f_2(\vect{x})}{x_2} & \cdots & \pd{f_2(\vect{x})}{x_n} \\
		\vdots & \vdots & \ddots & \vdots \\
		\pd{f_n(\vect{x})}{x_1} & \pd{f_n(\vect{x})}{x_2} & \cdots & \pd{f_n(\vect{x})}{x_n}
	\end{pmatrix}
\end{equation*}

For the roots given by $\vect{f} = \vect{0}$, the update rule becomes

\begin{equation} \label{updaterule}
	\vect{x}_{n+1} := \vect{x}_n - \vect{J}^{-1}(\vect{x}_n)\vect{f}(\vect{x}_n)	
\end{equation}

If the function $\vect{f}$ is itself the gradient of a function $R(\vect{x}): \R^n \rightarrow \R$, then the update rule becomes

\begin{equation} \label{updateruledog}
	\vect{x}_{n+1} := \vect{x}_n - \vect{H}_R^{-1}(\vect{x}_n)\nabla R(\vect{x}_n)
\end{equation}

where $\nabla R$ is the gradient of $R$ and $\vect{H}_R$ is the Hessian.
It is described in the appendix how this process is equivalent to iteratively optimizing a direct second order approximation of the response function $R$.

\begin{comment}
\section{A note on implementation}
Rather than being incorporated into mathematical formalism, the constraint on stimulus amplitude can also be implemented by re-normalizing the stimulus after every iteration of the optimizer. Geometrically, this has the effect of moving the new point $\vect{x}_{n+1}$ onto the sphere $\sum_n{x_n^2} = 1$ without changing the direction of the vector. \par
\hfill \newline
Additional, more general, constraints on the Newton optimization method regard properties of the Hessian matrix.
From \eqref{updateruledog} it can be seen that the response function Hessian must be invertible, that is

\begin{equation}
	\text{det}(\vect{H}_R) \neq 0
\end{equation}

In the case of \eqref{objmat}, it can be seen from \eqref{objhessian} that $\text{det}(\vect{H}_R) = 0$ if any term in the impulse-response kernel matrix $\hat{\vect{G}}$ is $0$. Thus invertibility can be guaranteed by adding a small number to all elements of the diagonal.

Further, if a maximum is required, the Hessian must be negative definite (all eigenvalues are $< 0$).
The first condition implies that the algorithm will fail if $\vect{x}$ falls on an inflection point, and the second condition is necessary for the search direction to point towards a maximum. 
Both of these conditions can be dealt with by re-intializing the optimization from a new starting point when one of the conditions is violated.
This method should be implemented even when the algorithm successfully terminates, as the search method does not guarantee that a global maximum will be found. 
\end{comment}

\section{Appendix} \label{appendix}

\subsection{A note on the partial derivatives of scalar-valued functions}
The following convention for the gradient ($\nabla$), Jacobian ($\vect{J}$), and Hessian($\vect{H}$) of a function $f: \R^n \rightarrow \R$ is adopted

\begin{align}
	\pd{f(\vect{x})}{\vect{x}} := \quad \nabla f & = 
	\begin{pmatrix}
		\pd{f(\vect{x})}{x_1} & \pd{f(\vect{x})}{x_2} & \cdots & \pd{f(\vect{x})}{x_n}
	\end{pmatrix}^{T} \label{gradient} \\[1em]
	\vect{J}_f & = 
	\begin{pmatrix}
		\pd{f(\vect{x})}{x_1} & \pd{f(\vect{x})}{x_2} & \cdots & \pd{f(\vect{x})}{x_n}
	\end{pmatrix} \label{jacobian} \\[1em]
	\vect{H}_f & = \pd{}{\vect{x}} \left( \pd{f(\vect{x})}{\vect{x}} \right) \nonumber \\
	& = \vect{J}_f(\nabla f)^{T} \label{hessian}
\end{align}	

\subsection{Identities for gradients of vectorized scalar forms}
Given the scalar $f(\vect{x}) = \vect{a}^{T}\vect{x}$, where

\begin{equation*}
	\vect{x}= 
	\begin{pmatrix}
		x_1 & x_2 & \cdots & x_n
	\end{pmatrix}^{T}
	\qquad \vect{a} = 
	\begin{pmatrix}
		a_1 & a_2 & \cdots & a_n
	\end{pmatrix}^{T}
\end{equation*}

$f(\vect{x})$ expands to

\begin{equation*}
	f(\vect{x}) = a_1x_1 + a_2x_2 + ... + a_nx_n
\end{equation*}

And by \eqref{gradient}

\begin{equation*}
	\pd{f(\vect{x})}{\vect{x}} = 
	\begin{pmatrix}
		a_1 & a_2 & \cdots & a_n
	\end{pmatrix}^{T} 
	= \vect{a}
\end{equation*}

Because $\vect{a}^{T}\vect{x} = \vect{x}^{T}\vect{a}$,

\begin{equation} \label{scalargradvv}
	\pd{\vect{a}^{T}\vect{x}}{\vect{x}} = \pd{\vect{x}^{T}\vect{a}}{\vect{x}} = \vect{a} \\[2em]
\end{equation} 

Given the scalar $f(\vect{x}) = \vect{x}^{T}\vect{A}\vect{x}$, where

\begin{equation*}
	\vect{x}= 
	\begin{pmatrix}
		x_1 \\ 
		x_2 \\ 
		\vdots \\ 
		x_n
	\end{pmatrix}
	\qquad \vect{A} = 
	\begin{pmatrix}
		a_{11} & a_{12} & \cdots & a_{1n} \\
		a_{21} & a_{22} & \cdots & a_{2n} \\
		\vdots & \vdots & \ddots & \vdots \\
		a_{n1} & a_{n2} & \cdots & a_{nn}
	\end{pmatrix}
\end{equation*}

$f(\vect{x})$ expands to

\begin{align*}
	f(\vect{x}) = \, & a_{11}x_1^2 + a_{12}x_2x_1 + ... + a_{1n}x_nx_1 + \\
				  & a_{21}x_1x_2 + a_{22}x_2^2 + ... + a_{2n}x_nx_2 + ... \, + \\
				  & a_{11}x_1x_n + a_{12}x_2x_n + ... + a_{nn}x_n^2
\end{align*}

The partial derivative with respect to $\vect{x}$ is given by \eqref{gradient}, where

\begin{equation*}
	\begin{matrix}
		\pd{f(\vect{x})}{x_1} = (a_{11} + a_{11})x_1 + (a_{12} + a_{21})x_2 + ... + 
							   	(a_{1n} + a_{n1})x_n \\
		\pd{f(\vect{x})}{x_2} = (a_{21} + a_{12})x_1 + (a_{22} + a_{22})x_2 + ... + 
							   	(a_{2n} + a_{n2})x_n \\
		\vdots \\
		\pd{f(\vect{x})}{x_n} = (a_{n1} + a_{1n})x_1 + (a_{n2} + a_{2n})x_2 + ... + 
							   	(a_{nn} + a_{nn})x_n 
	\end{matrix}
\end{equation*}

In this form, it is clear that

\begin{align*}
	\pd{f(\vect{x})}{\vect{x}} & = 
	\begin{pmatrix}
		(a_{11} + a_{11}) & (a_{12} + a_{21}) & \cdots & (a_{1n} + a_{n1}) \\
		(a_{21} + a_{12}) & (a_{22} + a_{22}) & \cdots & (a_{2n} + a_{n2}) \\
		\vdots & \vdots & \ddots & \vdots \\
		(a_{n1} + a_{1n}) & (a_{n2} + a_{2n}) & \cdots & (a_{nn} + a_{nn}) 
	\end{pmatrix} 
	\begin{pmatrix}
		x_1 \\ 
		x_2 \\ 
		\vdots \\ 
		x_n
	\end{pmatrix} \\
	& = (\vect{A} + \vect{A}^{T})\vect{x}
\end{align*}

Thus

\begin{equation} \label{scalargradqf}
	\pd{\vect{x}^{T}\vect{Ax}}{\vect{x}} = (\vect{A} + \vect{A}^{T})\vect{x}
\end{equation}

\subsection{Equivalence of second-order approximation and Newton optimization methods}
The second-order Taylor expansion of a twice-differentiable function $f$ at a point $x \rightarrow a$ is given by

\begin{equation} \label{taylor1d}
	f(x) \approx f(a) + f'(a)(x - a) + \frac{1}{2}f''(a)(x - a)^2
\end{equation}

For $f: \R^2 \rightarrow \R$, this becomes

\begin{align} \label{taylor2d}
	f(x,y) \approx \, & f(a,b) + f_x(a,b)(x - a) + f_y(a,b)(y - b) \, + \nonumber \\
					   & \frac{1}{2}f_{xx}(a,b)(x - a)^2 + \frac{1}{2}f_{xy}(a,b)(x - a)(y - b) + 
						 \frac{1}{2}f_{yy}(a,b)(y - a)^2 
\end{align}
 
Vectorizing \eqref{taylor2d} and generalizing to $n$ dimensions, $f: \R^n \rightarrow \R$ is approximated by

\begin{equation} \label{taylornd}
	f(\vect{x}) \approx f(\vect{a}) + \vect{J}(\vect{a})(\vect{x} - \vect{a}) + 
						 \frac{1}{2}(\vect{x} - \vect{a})^{T}\vect{H}(\vect{a})(\vect{x} - \vect{a})
\end{equation}

Where $\vect{J}$ is the Jacobian of $f$, and $\vect{H}$ is the Hessian (which is assumed to be symmetrical by the Schwarz-Clairaut theorem). 
Expanding, and collecting all terms dependent on $\vect{x}$

\begin{equation*}
	f(\vect{x}) \approx \frac{1}{2}\vect{x}^{T}\vect{Hx} - \frac{1}{2}\vect{x}^{T}\vect{Ha} -
						 \frac{1}{2}\vect{a}^{T}\vect{Hx} + \vect{Jx} + ...
\end{equation*}

In order to find stationary points, the derivative is taken with respect to $\vect{x}$. 
By \eqref{scalargradqf} and \eqref{scalargradvv} respectively, the partial derivatives of the first and fourth terms with respect to $\vect{x}$ become

\begin{align*}
	\pd{\vect{x}^{T}\vect{Hx}}{\vect{x}} & = (\vect{H} + \vect{H}^{T})\vect{x} \\
	\pd{\vect{Jx}}{\vect{x}} = \pd{(\vect{J}^{T})^{T}\vect{x}}{\vect{x})} & = \vect{J}^{T}
\end{align*}
Next, it is noted that

\begin{align*}
	\vect{A}^{T}\vect{B}^{T} & = (\vect{BA})^{T} \\
	\vect{H} & = \vect{H}^{T}
\end{align*}
	
Thus, if $\vect{z} = \vect{Ha} \enspace \rightarrow \enspace \vect{a}^{T}\vect{H} = \vect{z}^{T}$, and

\begin{equation*}
	\pd{\vect{z}^{T}\vect{x}}{\vect{x}} = \pd{\vect{x}^{T}\vect{z}}{\vect{x}} = 
	\vect{z} = \vect{Ha}
\end{equation*}

Finally, assembling the partial derivatives of all terms

\begin{equation}
	\pd{f(\vect{x})}{\vect{x}} = \vect{Hx} - \vect{Ha} + \vect{J}^{T}
\end{equation}

Setting

\begin{equation*}
	\pd{f(\vect{x})}{\vect{x}} = 0 \quad \longrightarrow \quad 
	\vect{x} = \vect{a} - \vect{H}^{-1}\vect{J}^{T}
\end{equation*}

Algorithmically,

\begin{equation} \label{updaterule2}
	\vect{x}_{n+1} := \vect{x}_n - \vect{H}^{-1}(\vect{x}_n)\vect{J}^{T}(\vect{x}_n) 
\end{equation}


\bibliography{references.bib}

\end{document}